{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/09eesx/forensic-emotion-ai/blob/main/data_cleaningandtokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n",
        "!pip install accelerate -U\n",
        "!pip install -U transformers\n"
      ],
      "metadata": {
        "id": "F-dObDAKep7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "import random\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from scipy.sparse import hstack  # To combine sparse matrices\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4vSqmmM4bZAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ana_klasor = \"/content/drive/MyDrive/data/raw_data/data_aiagent/raw_data/data_aiagent/doic_elli\"\n",
        "etiket_dosyasi = \"/content/drive/MyDrive/data/raw_data/data_aiagent/raw_data/data_aiagent/doic_elli/train_split_Depression_AVEC2017 (2).csv\""
      ],
      "metadata": {
        "id": "t0jD7TjtbalJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Etiketleri oku\n",
        "etiket_df = pd.read_csv(etiket_dosyasi)\n",
        "etiket_df[\"Participant_ID\"] = etiket_df[\"Participant_ID\"].astype(int)\n",
        "\n",
        "tum_veriler = []\n",
        "\n",
        "for dosya_adi in os.listdir(ana_klasor):\n",
        "    if not dosya_adi.endswith(\"_TRANSCRIPT.csv\"):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        participant_id = int(dosya_adi.split(\"_\")[0])\n",
        "        transcript_yolu = os.path.join(ana_klasor, dosya_adi)\n",
        "\n",
        "        df = pd.read_csv(transcript_yolu, sep=\"\\t\", header=0)\n",
        "\n",
        "        if \"speaker\" not in df.columns or \"value\" not in df.columns:\n",
        "            print(f\"{participant_id} ‚Üí Ge√ßersiz format (eksik s√ºtun)\")\n",
        "            continue\n",
        "\n",
        "        df_participant = df[df[\"speaker\"] == \"Participant\"]\n",
        "        df_participant = df_participant[~df_participant[\"value\"].astype(str).str.contains(\"scrubbed\", na=False)]\n",
        "        df_participant = df_participant.dropna(subset=[\"value\"])\n",
        "\n",
        "        full_text = \" \".join(df_participant[\"value\"].tolist())\n",
        "\n",
        "        etiket_satiri = etiket_df[etiket_df[\"Participant_ID\"] == participant_id]\n",
        "        if etiket_satiri.empty:\n",
        "            print(f\"{participant_id} ‚Üí Etiket bulunamadƒ±\")\n",
        "            continue\n",
        "\n",
        "        phq_score = etiket_satiri[\"PHQ8_Score\"].values[0]\n",
        "        depresyon_etiketi = etiket_satiri[\"PHQ8_Binary\"].values[0]\n",
        "\n",
        "        tum_veriler.append({\n",
        "            \"participant_id\": participant_id,\n",
        "            \"full_text\": full_text,\n",
        "            \"phq_score\": phq_score,\n",
        "            \"depression_label\": depresyon_etiketi\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{dosya_adi} ‚Üí Hata: {e}\")\n",
        "\n",
        "# Kaydet\n",
        "veri_df = pd.DataFrame(tum_veriler)\n",
        "veri_df.to_csv(\"cleaned_daic_woz_texts.csv\", index=False)\n",
        "\n",
        "print(\"Tamamlandƒ±. Toplam katƒ±lƒ±mcƒ±:\", len(veri_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcEZ1_ykbdcY",
        "outputId": "38d37527-d3b5-44ed-f698-8e3aad362732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300 ‚Üí Etiket bulunamadƒ±\n",
            "301 ‚Üí Etiket bulunamadƒ±\n",
            "302 ‚Üí Etiket bulunamadƒ±\n",
            "308 ‚Üí Etiket bulunamadƒ±\n",
            "309 ‚Üí Etiket bulunamadƒ±\n",
            "311 ‚Üí Etiket bulunamadƒ±\n",
            "323 ‚Üí Etiket bulunamadƒ±\n",
            "329 ‚Üí Etiket bulunamadƒ±\n",
            "332 ‚Üí Etiket bulunamadƒ±\n",
            "334 ‚Üí Etiket bulunamadƒ±\n",
            "335 ‚Üí Etiket bulunamadƒ±\n",
            "337 ‚Üí Etiket bulunamadƒ±\n",
            "346 ‚Üí Etiket bulunamadƒ±\n",
            "‚úÖ Tamamlandƒ±. Toplam katƒ±lƒ±mcƒ±: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxtpkuiBapa4",
        "outputId": "2d852457-c827-4dd3-e817-7516b00c1968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Toplam √∂rnek sayƒ±sƒ±: 95076\n",
            "üìÅ Kaydedildi: datasets/train_cleaned.csv\n",
            "üéØ Etiket Daƒüƒ±lƒ±mƒ±:\n",
            "label\n",
            "√∂fke                 19961\n",
            "normal               16360\n",
            "depresyon            15414\n",
            "intihar eƒüilimi      10652\n",
            "anksiyete             8798\n",
            "√ºzg√ºnl√ºk              6947\n",
            "sevin√ß                5050\n",
            "umutlu                4390\n",
            "bipolar               2777\n",
            "stres                 2587\n",
            "ki≈üilik bozukluƒüu     1077\n",
            "hakaret                528\n",
            "tehdit                 478\n",
            "nefret s√∂ylemi          57\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ortak etiketler\n",
        "LABELS = [\n",
        "    \"depresyon\", \"intihar eƒüilimi\", \"anksiyete\", \"stres\", \"ki≈üilik bozukluƒüu\", \"bipolar\",\n",
        "    \"√∂fke\", \"hakaret\", \"tehdit\", \"nefret s√∂ylemi\", \"√ºzg√ºnl√ºk\", \"umutlu\", \"sevin√ß\", \"normal\"\n",
        "]\n",
        "\n",
        "def load_empathetic(path):\n",
        "    mapping = {\n",
        "        \"sad\": \"√ºzg√ºnl√ºk\",\n",
        "        \"ashamed\": \"√ºzg√ºnl√ºk\",\n",
        "        \"disappointed\": \"√ºzg√ºnl√ºk\",\n",
        "        \"angry\": \"√∂fke\",\n",
        "        \"furious\": \"√∂fke\",\n",
        "        \"anxious\": \"anksiyete\",\n",
        "        \"afraid\": \"anksiyete\",\n",
        "        \"hopeful\": \"umutlu\",\n",
        "        \"trusting\": \"umutlu\",\n",
        "        \"joyful\": \"sevin√ß\",\n",
        "        \"proud\": \"sevin√ß\"\n",
        "    }\n",
        "    df = pd.read_csv(path, on_bad_lines='skip', engine='python')\n",
        "    df = df.rename(columns={\"utterance\": \"text\", \"context\": \"original_label\"})\n",
        "    df[\"label\"] = df[\"original_label\"].map(mapping)\n",
        "    df = df[[\"text\", \"label\"]].dropna()\n",
        "    return df\n",
        "\n",
        "\n",
        "# 2. Jigsaw\n",
        "def load_jigsaw(path):\n",
        "    df_jigsaw = pd.read_csv(path, on_bad_lines='skip', engine='python')\n",
        "    def map_jigsaw(row):\n",
        "        if row[\"threat\"] == 1:\n",
        "            return \"tehdit\"\n",
        "        elif row[\"toxic\"] == 1:\n",
        "            return \"√∂fke\"\n",
        "        elif row[\"insult\"] == 1:\n",
        "            return \"hakaret\"\n",
        "        elif row[\"identity_hate\"] == 1:\n",
        "            return \"nefret s√∂ylemi\"\n",
        "        else:\n",
        "            return None\n",
        "    df_jigsaw[\"label\"] = df_jigsaw.apply(map_jigsaw, axis=1)\n",
        "    df_jigsaw = df_jigsaw.rename(columns={\"comment_text\": \"text\"})\n",
        "    df_jigsaw = df_jigsaw[[\"text\", \"label\"]].dropna()\n",
        "    return df_jigsaw\n",
        "\n",
        "# 3. Combined_data\n",
        "def load_combined(path):\n",
        "    df_comb = pd.read_csv(path)\n",
        "    mapping = {\n",
        "        \"Anxiety\": \"anksiyete\",\n",
        "        \"Normal\": \"normal\",\n",
        "        \"Depression\": \"depresyon\",\n",
        "        \"Suicidal\": \"intihar eƒüilimi\",\n",
        "        \"Stress\": \"stres\",\n",
        "        \"Bipolar\": \"bipolar\",\n",
        "        \"Personality disorder\": \"ki≈üilik bozukluƒüu\"\n",
        "    }\n",
        "    df_comb = df_comb.rename(columns={\"status\": \"original_label\", \"statement\": \"text\"})\n",
        "    df_comb[\"label\"] = df_comb[\"original_label\"].map(mapping)\n",
        "    df_comb = df_comb[[\"text\", \"label\"]].dropna()\n",
        "    return df_comb\n",
        "\n",
        "# 4. DAIC-WOZ\n",
        "def load_daic(path):\n",
        "    df_daic = pd.read_csv(path)\n",
        "    df_daic[\"label\"] = df_daic[\"depression_label\"].apply(lambda x: \"depresyon\" if x == 1 else \"normal\")\n",
        "    df_daic = df_daic.rename(columns={\"full_text\": \"text\"})\n",
        "    df_daic = df_daic[[\"text\", \"label\"]].dropna()\n",
        "    return df_daic\n",
        "\n",
        "# T√ºm veri setlerini oku\n",
        "empathetic_df = load_empathetic(\"/content/drive/MyDrive/data/raw_data/data_aiagent/raw_data/empatheticdialogues/empatheticdialogues/train.csv\")\n",
        "jigsaw_df = load_jigsaw(\"/content/drive/MyDrive/data/raw_data/data_aiagent/raw_data/jigsaw-toxic-comment-classification-challenge/train.csv/train.csv\")\n",
        "combined_df = load_combined(\"/content/drive/MyDrive/data/raw_data/data_aiagent/raw_data/data_aiagent/Combined Data.csv\")\n",
        "daic_df = load_daic(\"/content/cleaned_daic_woz_texts.csv\")\n",
        "\n",
        "# Hepsini birle≈ütir\n",
        "all_data = pd.concat([empathetic_df, jigsaw_df, combined_df, daic_df], ignore_index=True)\n",
        "\n",
        "# Ge√ßerli etiketler dƒ±≈üƒ±ndakileri at\n",
        "all_data = all_data[all_data[\"label\"].isin(LABELS)]\n",
        "\n",
        "\n",
        "\n",
        "# Kaydet\n",
        "all_data.to_csv(\"/content/drive/MyDrive/data/train_cleaned2.csv\", index=False)\n",
        "print(f\"‚úÖ Toplam √∂rnek sayƒ±sƒ±: {len(all_data)}\")\n",
        "print(\"üìÅ Kaydedildi: datasets/train_cleaned.csv\")\n",
        "print(\"üéØ Etiket Daƒüƒ±lƒ±mƒ±:\")\n",
        "print(all_data['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##oversampling"
      ],
      "metadata": {
        "id": "cJBTD9POeOJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ 2. Manuel oversampling (dengesiz sƒ±nƒ±flar)\n",
        "print(\" Sƒ±nƒ±f sayƒ±larƒ± (√∂nce):\")\n",
        "print(all_data[\"label\"].value_counts())\n",
        "\n",
        "oversample_labels = [\"ki≈üilik bozukluƒüu\",\"hakaret\",\"tehdit\"]\n",
        "for label in oversample_labels:\n",
        "    target_df = all_data[all_data[\"label\"] == label]\n",
        "    for _ in range(20):  # 20 kat √ßoƒüalt\n",
        "        all_data = pd.concat([all_data, target_df], ignore_index=True)\n",
        "\n",
        "print(\"\\n Sƒ±nƒ±f sayƒ±larƒ± (sonra):\")\n",
        "print(all_data[\"label\"].value_counts())\n",
        "\n",
        "\n",
        "# Her sƒ±nƒ±ftan en az 150 √∂rnek al\n",
        "df_balanced = all_data.groupby(\"label\").apply(lambda x: x.sample(min(150, len(x)), random_state=42)).reset_index(drop=True)\n",
        "\n",
        "# Her sƒ±nƒ±ftan 10000 √∂rnek al\n",
        "df_filtered = all_data.groupby(\"label\").apply(lambda x: x.sample(min(len(x), 10000), random_state=42))\n",
        "df_filtered = df_filtered.reset_index(drop=True)\n",
        "\n",
        "# Yeni sƒ±nƒ±f daƒüƒ±lƒ±mƒ±nƒ± g√∂relim\n",
        "print(\" Yeni sƒ±nƒ±f daƒüƒ±lƒ±mƒ±:\")\n",
        "print(df_filtered[\"label\"].value_counts())\n",
        "\n",
        "# Kaydet\n",
        "df_filtered.to_csv(\"/content/drive/MyDrive/data/train_cleaned_balanced2.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1tdDaqLa8os",
        "outputId": "cf91fe4e-6884-4389-d69f-3d51b1e57f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Sƒ±nƒ±f sayƒ±larƒ± (√∂nce):\n",
            "label\n",
            "√∂fke                 19961\n",
            "normal               16360\n",
            "depresyon            15414\n",
            "intihar eƒüilimi      10652\n",
            "anksiyete             8798\n",
            "√ºzg√ºnl√ºk              6947\n",
            "sevin√ß                5050\n",
            "umutlu                4390\n",
            "bipolar               2777\n",
            "stres                 2587\n",
            "ki≈üilik bozukluƒüu     1077\n",
            "hakaret                528\n",
            "tehdit                 478\n",
            "nefret s√∂ylemi          57\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìà Sƒ±nƒ±f sayƒ±larƒ± (sonra):\n",
            "label\n",
            "ki≈üilik bozukluƒüu    22617\n",
            "√∂fke                 19961\n",
            "normal               16360\n",
            "depresyon            15414\n",
            "hakaret              11088\n",
            "intihar eƒüilimi      10652\n",
            "tehdit               10038\n",
            "anksiyete             8798\n",
            "√ºzg√ºnl√ºk              6947\n",
            "sevin√ß                5050\n",
            "umutlu                4390\n",
            "bipolar               2777\n",
            "stres                 2587\n",
            "nefret s√∂ylemi          57\n",
            "Name: count, dtype: int64\n",
            "üß™ Yeni sƒ±nƒ±f daƒüƒ±lƒ±mƒ±:\n",
            "label\n",
            "depresyon            10000\n",
            "hakaret              10000\n",
            "intihar eƒüilimi      10000\n",
            "ki≈üilik bozukluƒüu    10000\n",
            "normal               10000\n",
            "tehdit               10000\n",
            "√∂fke                 10000\n",
            "anksiyete             8798\n",
            "√ºzg√ºnl√ºk              6947\n",
            "sevin√ß                5050\n",
            "umutlu                4390\n",
            "bipolar               2777\n",
            "stres                 2587\n",
            "nefret s√∂ylemi          57\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenization\n"
      ],
      "metadata": {
        "id": "tCzXPf6zeHky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_patterns(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "    # Remove markdown-style links\n",
        "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
        "    # Remove handles (that start with '@')\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove punctuation and other special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply the function to the 'statement' column\n",
        "all_data['text'] = all_data['text'].apply(remove_patterns)\n"
      ],
      "metadata": {
        "id": "GHhBBL__hEgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaOmY85hiyG5",
        "outputId": "a6f28142-aa20-48fc-b253-2775583ff78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply word_tokenize to each element in the 'statement' column\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "all_data['tokens'] = all_data['text'].apply(word_tokenize)\n"
      ],
      "metadata": {
        "id": "tvRUSoUdhq9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to stem tokens and convert them to strings\n",
        "def stem_tokens(tokens):\n",
        "    return ' '.join(stemmer.stem(str(token)) for token in tokens)\n",
        "\n",
        "# Apply the function to the 'tokens' column\n",
        "all_data['tokens_stemmed'] = all_data['tokens'].apply(stem_tokens)\n"
      ],
      "metadata": {
        "id": "kUVG2lcHjagi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ 3. Label encoding\n",
        "le = LabelEncoder()\n",
        "all_data[\"label_encoded\"] = le.fit_transform(all_data[\"label\"])\n",
        "all_data.label_encoded.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q1MiZgfkXX9",
        "outputId": "c54aae15-b7b1-45ae-f72c-0b336fb7d7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  8, 12, 13, 11, 10,  3,  6,  7,  2,  4,  9,  1,  5])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['num_of_characters'] = all_data['text'].apply(len)\n"
      ],
      "metadata": {
        "id": "hQ51dBJWk7ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def count_sentences(text):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s for s in sentences if s.strip() != '']  # bo≈ülarƒ± sil\n",
        "    return len(sentences)\n",
        "\n",
        "all_data['num_of_sentences'] = all_data['text'].apply(count_sentences)\n"
      ],
      "metadata": {
        "id": "lxqSkVDVljxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = all_data[['tokens_stemmed',\t'num_of_characters',\t'num_of_sentences']]\n",
        "y = all_data[['label_encoded']]"
      ],
      "metadata": {
        "id": "N2HOLTWrlzoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
      ],
      "metadata": {
        "id": "4Lam5wfimMT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Initialize TF-IDF Vectorizer and fit/transform on the 'tokens' column\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=50000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train['tokens_stemmed'])\n",
        "X_test_tfidf = vectorizer.transform(X_test['tokens_stemmed'])\n",
        "\n",
        "# 2. Extract numerical features\n",
        "X_train_num = X_train[['num_of_characters', 'num_of_sentences']].values\n",
        "X_test_num = X_test[['num_of_characters', 'num_of_sentences']].values\n",
        "\n",
        "# 3. Combine TF-IDF features with numerical features\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_num])\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_num])\n",
        "\n",
        "print('Number of feature words: ', len(vectorizer.get_feature_names_out()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq3jQRbrmOVa",
        "outputId": "1e98755f-9be5-4ff2-e7a8-842e6f56c7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of feature words:  50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_combined.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RacLj1l3mU_s",
        "outputId": "61dd8cdb-6048-48d6-ef59-8c94b6e5057c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109388, 50002)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Random Over-Sampling on the vectorized data\n",
        "ros = RandomOverSampler(random_state=101)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_combined, y_train)"
      ],
      "metadata": {
        "id": "Hclmb-s6mgfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_resampled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em1z1TSYmhho",
        "outputId": "c60c74b2-e36e-44e0-f2c8-1482e9b6b7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(252490, 50002)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/annastasy/mental-health-sentiment-analysis-nlp-ml/notebook faydalanƒ±lmƒ±≈ütƒ±r"
      ],
      "metadata": {
        "id": "me5eIPeCnf7W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkX8wHjbdrwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}